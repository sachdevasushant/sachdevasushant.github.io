<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

 <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
    <title>Fast Algorithms via Continuous Methods : CSC 2421H (Fall 2017) : Sushant Sachdeva</title>
    <link rel="stylesheet" type="text/css" href="../../courses.css" media="screen, tv, projection" title="Default" />

    <meta http-equiv="content-type" content="application/xhtml+xml; charset=UTF-8" />
    <meta name="generator" content="Sushant Sachdeva" />
    <meta name="description" content="Home page of Sushant Sachdeva" />
    <meta name="keywords" content="Sushant,Sachdeva" />
</head>
<body>


<!--Script for Google Analytics-->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-3519710-6', 'auto');
  ga('send', 'pageview');

</script>
<!-- End script for Google Analytics-->

<SCRIPT language="JavaScript"> 
  function toggle(aa) {
    state=document.getElementById(aa).style.display
    if (state == '' || state == 'none') {
	  document.getElementById(aa).style.display='block';
    }
    if (state == 'block') {
	  document.getElementById(aa).style.display='none';
    }
    element = document.getElementById("main");
    element.style.display='none';
    element.style.display='block';
  }
</SCRIPT>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<div id="page">
  <div id="header">
    CSC 2421H : Fast Algorithms via Continuous Methods
  </div>

  <div id = "content">
    <div id="main">
      <H3>Objective</H3>
      <p> This course will focus on research topics in modern
  algorithm design, including tools, techniques, and their
  applications to the design of algorithms for fundamental problems in
  theoretical computer science.  In this offering, the focus will be
  on methods from continuous optimization and analysis, and their
  applications to the design of fast algorithms for fundamental
  problems in theoretical computer science and numerical linear
  algebra.</p>
      
      <H3>Logistics</H3>
      <p>
      Where: BA 4010 </br>
      When: T 2-4pm </br>
      Instructor: <a href="../../index.html"
      target="_blank">Sushant Sachdeva</a></br>
      Office hours: By appointment (email me, or catch me after class)</br>
      </p>

      <H3>Prerequisites</H3>
      <p>A good understanding of undergraduate algorithms, Linear
      algebra, probability, and some vector calculus. Most of all,
      this course will assume good mathematical maturity as the course
      will be theoretical and mathematical in nature.</p>

      <H3>Syllabus</H3>
      <p>Here is a tentative list of topics we plan to cover in the course:</br>
      * Solving systems of linear equations efficiently </br>
 * Basic techniques in optimization, such as Gradient descent </br>
* Concentration bounds </br>
* Low dimensional embeddings </br>
* Linear/Semidefinite programming, and duality </br>
* Applications of convex programming to approximation algorithms </br>
In the remaining part of the course, the students will read and present
      research papers related to the above topics. </p>

      <H3>Notes</H3>
      <p>Click on the topic for scribe notes and additional readings
      <ol>
	<li>
	<div id="title"><a
	href="javascript:toggle('notes1')">Linear Regression via Gradient Descent</a></div>
	<div id="notes1" class="abstract">
	  <p>12 Sep <br/> <a href="../15s-cpsc665/notes/linsys.pdf"
				target="_blank">[Notes from Spring '15]</a></br>
	  We formulated the problem of linear regression and proved convergence bounds on solving it using
	  Gradient decent.
	  <p> Additional readings:
	    Section 9.1 in my survey: <a href="http://www.cs.yale.edu/homes/sachdeva/pubs/fast-algos-via-approx-theory.pdf" target="_blank">Faster Algorithms via Approximation Theory</a>
	  </p>
	</div>
	</li>

	<li>
	<div id="title"><a
	href="javascript:toggle('notes2')">Solving linear systems via
	Conjugate Gradient method</a></div>
	<div id="notes2" class="abstract">
	  <p>19 Sep <br/> <a href="../15s-cpsc665/notes/linsys.pdf"
				target="_blank">[Notes from Spring '15]</a></br>
	  We studied the Conjugate Gradient method that does better
				than Gradient descent by picking the
	  best vector in the Krylov subspace.
	  <p> Additional readings: Section 9.2 in my
	    survey: <a href="http://www.cs.yale.edu/homes/sachdeva/pubs/fast-algos-via-approx-theory.pdf"
	    target="_blank">Faster Algorithms via Approximation
	    Theory</a>
	  </p>
	</div>
	</li>
	
	
	<li>
	<div id="title"><a
	href="javascript:toggle('notes3')">
	Solving Laplacian Systems via Randomized Kaczmarz</a></div>
	<div id="notes3" class="abstract">
	  <p>26 Sep<br/> <a href="../15s-cpsc665/notes/lapsolvers.pdf"
	  target="_blank">[Notes from Spring '15]</a></br> We
	  introduced Laplacian systems, and proceeded to show how we
	  can solve them in near-linear time using Randomized Kaczmarz
	  method and low-stretch spanning trees.
	  	  <p> Additional readings: Chapter 14 in Nisheeth
	    Vishnoi's survey
	     <a href="http://theory.epfl.ch/vishnoi/Lxb-Web.pdf"
	    target="_blank">'Lx=b'
	  </a>. This survey is a good source for a lot of material
	  around Laplacian
	  solvers. <a href="../15s-cpsc665/notes/low-stretch.pdf"
	  target="_blank">Here</a> are notes for low-stretch spanning
	  tree based on <a href="https://arxiv.org/abs/cs/0411064"
	  target="_blank">this paper</a> by Elkin, Emek, Spielman, and
	  Teng.
	</div>
	</li>

		
	
	<li>
	<div id="title"><a
	href="javascript:toggle('notes4')">
	Linear Programming and Approximation Algorithms</a></div>
	<div id="notes4" class="abstract">
	  <p>3/10 Oct<br/> <a href="../15s-cpsc665/notes/lp.pdf"
	  target="_blank">[Notes from Spring '15]</a></br> We
	  introduced Linear programming (LP) and approximation
	  algorithms, and used LP to design some simple approximation
	  algorithms. <a href="max-sat-inequality.jpg" target="_blank">Proof</a> of the
	  final inequality in MAX-SAT approximation.
	</div>
	</li>

	
	<li>
	<div id="title"><a
	href="javascript:toggle('notes5')">
	Convexity and Inequalities</a></div>
	<div id="notes5" class="abstract">
	  <p>10 Oct<br/> <a href="../15s-cpsc665/notes/convexity.pdf"
	  target="_blank">[Notes from Spring '15]</a></Br> We
	  introduced several equivalent notions of convexity and
	  proved Jensen's inequality and AM/GM inequality.
	</div>
	</li>


		
	<li>
	<div id="title"><a
	href="javascript:toggle('notes6')">Max-Cut and Semidefinite Programming</a></div>
	<div id="notes6" class="abstract">
	  <p> 12 Oct<br/>
	    <a href="../15s-cpsc665/notes/sdp1.pdf"
	       target="_blank">[Notes1 from Spring '15]
	    </a>
	    <a href="../15s-cpsc665/notes/sdp2.pdf"
	       target="_blank">[Notes2 from Spring '15]</a>
	    <br/> In this lecture, we introduced the Max-Cut problem,
	    and defined semidefinite programs (SDP). We interpreted
	    SDPs as vector relaxations, and gave the
	    Goemens-Williamson vector relaxation for Max-Cut, and
	    rounded it using their randomized hyperplane rounding.
	</div>
	</li>

	<li>
	  <div id="title"><a href="javascript:toggle('notes7')">
	      Stronger Relaxations and Balanced Separater</a></div>
	<div id="notes7" class="abstract">
	  <p>	24 Oct<br/> </a>
	    <a href="../15s-cpsc665/notes/sdp2.pdf"
	       target="_blank">[Notes1 from Spring '15]</a>
	    <a href="../15s-cpsc665/notes/balsep.pdf"
	  target="_blank">[Notes2 from Spring '15]</a> </br> In this
	  lecture, we introduced the Balanced Separater problem. We
	  presented the idea of strengthening relaxations, and gave a
	  strengthened SDP for Balanced Separator (from the work of
	  Arora-Rao-Vazirani) using triangle inequalities, and proved
	  that it gives us a \(\log n\) approximation.
	</div>
	</li>

	<li>
	  <div id="title"><a href="javascript:toggle('notes8')">
	      Concentration Bounds</a></div>
	<div id="notes8" class="abstract">
	  <p>	31 Oct<br/> </a>
	    <a href="../15s-cpsc665/notes/concentration.pdf"
	       target="_blank">[Notes from Spring '15]</a>
	       Concentration bounds allow us to show that a random
	       variable, under certain conditions, lies near its mean
	       with high probability. In this lecture, we proved the
	       Markov and Chebyshev inequalities and one version of
	       Chernoff bounds.
	</div>
	</li>

	</ol>

	<H3>Principle readings</H3>
      <p>There will be no textbook for the course, but we will use several online resources as reading material, in addition to the scribe notes. Some of the relevant courses/resources are:</br>
* <a href="http://www.cs.princeton.edu/courses/archive/fall13/cos521/" target="_blank">Course notes from Advanced Algorithms</a> by Sanjeev Arora </br>
* <a href="http://web.stanford.edu/~boyd/cvxbook/" target="_blank">Convex Optimization</a> by Boyd and Vanderberghe </br>
* <a href="http://www.designofapproxalgs.com/" target="_blank"> The design of approximation algorithms</a> by D. Williamson, D. Shmoys </br>
* <a href="http://wwwusers.di.uniroma1.it/ale/Papers/master.pdf" target="_blank">Concentration of Measure for the Analysis of Randomised Algorithms</a> by D.Dubhashi, A.Panconesi </br>
* <a href="http://www.cs.yale.edu/homes/sachdeva/pubs/fast-algos-via-approx-theory.pdf" target="_blank">Faster Algorithms via Approximation Theory</a> by S. Sachdeva, N. Vishnoi </br>
* <a href="http://research.microsoft.com/en-us/um/people/nvishno/site/Lxb-Web.pdf" target="_blank">Lx=b</a> by N. Vishnoi </br>
* <a href="http://ocw.mit.edu/courses/mathematics/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/lecture-notes/" target="_blank">Course notes from 'An Algorithmist's toolkit'</a> by Jonathan Kelner</p>
    </div>
  </div>


</div>
</body>
</html>
