<!doctype html>
<html lang="en">

<head>
<meta charset="UTF-8">
<title>Sushant Sachdeva</title>
<meta name="description" content="Sushant Sachdeva's homepage">
<meta name="keywords" content="research,yale,sushant,sachdeva,theory,cs">
<meta name="author" content="Sushant Sachdeva">
<link rel="stylesheet" href="newstyle.css" type="text/css" media="screen" />
</head>

<body>

<!--Script for Google Analytics-->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-3519710-6', 'auto');
  ga('send', 'pageview');

</script>
<!-- End script for Google Analytics-->

<!--Script for toggling abstracts-->
<SCRIPT  type="text/javascript"> 
  function toggle(aa) {
    state=document.getElementById(aa).style.display
    if (state == '' || state == 'none') {
	  document.getElementById(aa).style.display='block';
    }
    if (state == 'block') {
	  document.getElementById(aa).style.display='none';
    }
  }
</SCRIPT>
<!--End Script for toggling abstracts-->

<!--Script for mathjax-->
<script type="text/javascript"
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
<!--End Script for toggling abstracts-->

<div id="container">
  <header>


  <div id="nav">
  <div id="mainmenu">
  <div id="banner">
    <h1><a href="index.html"><div class="active_page">Sushant Sachdeva</div></a></h1>
  </div><!--end banner-->
  <div id="menu">
  <ul>
    <li><a href="contact.html" >Contact</a></li>
    <li><a href="teaching.html" >Teaching</a></li>
    <li><a href="research.html">Research</a></li>
    <li><a href="app/cv-sachdeva.pdf" target="_blank">CV</a></li>
  </ul>
</div><!--menu end-->
</div><!--mainmenu end-->
<div id="submenu">
  <ul>
    <li></li>
    </ul>
  </div><!--submenu end-->
  </div><!--nav end-->
</header>


<div id="content">
<!--<div id="announcement">
    I am looking for an academic position starting Fall 2016.
    </div>-->
  <div id="business_card">
    <div id="img_left">
      <img src="new_me.jpg" width="200" alt="This is me!" />
    </div>

    <div id="affiliation">
      
      <div id="current_affiliation">
	Assistant Professor<br/>
	  University of Toronto
</div><!--current affiliation-->
<!--	<a href="http://cpsc.yale.edu/" target="_blank">Department of
      Computer Science</a><br/> -->
<!--	<a href="http://yale.edu/" target="_blank">Yale University</a><br/>
	Hosted by <a href="http://www.cs.yale.edu/homes/spielman/"
	target="_blank">Daniel Spielman</a><br/> -->
      
	<!--      <H3>Previous Affiliations</H3> -->

      <p>
	Assistant Professor, University of Toronto Mississauga<br>
	Faculty Affiliate, Vector Institute
      </p>
	
      <!-- <table> -->
      <!-- 	<tbody> -->
      <!-- 	<tr> -->
      <!-- 	  <td><span class="duration">Faculty Affiliate, Vector Institute</span></td> -->
      <!-- 	</tr> -->
      <!-- 	</tbody> -->
      <!-- </table> -->


      
      <table>	
	
	<tr>
	  <td><span class="duration">Fall 2019</span></td>
	  <td>Visitor, 
	    <a href="https://www.ias.edu/scholars/sushant-sachdeva"
	    target="_blank">Institute for Advanced Study</a>
	  </td>
	</tr>
	
	<tr>
	  <td><span class="duration">2016-2017</span></td>
	  <td>Research Scientist, 
	    <a href="https://research.google.com/" target="_blank">Google</a>
	  </td>
	</tr>

	<tr>
	  <td><span class="duration">2014-2016</span><br/> &nbsp; </td>
	  <td>
	    <a href="http://cpsc.yale.edu/" target="_blank">Department of Computer Science</a>,
	    <a href="http://www.yale.edu/" target="_blank">Yale University</a><br/>

	    Postdoctoral Associate, Hosted by <a href="http://www.cs.yale.edu/homes/spielman/"
	    target="_blank">Daniel Spielman</a>
	  </td>
	</tr>
	
	<tr>
	  <td><span class="duration">Fall 2013</span></td>
	  <td>Simons Fellow, <a href="http://simons.berkeley.edu/"
	    target="_blank">Simons Institute</a>,
	    <a href="http://www.berkeley.edu/" target="_blank">UC Berkeley</a>
	  </td>
	</tr>
	<!--   <a href="http://simons.berkeley.edu/programs/realanalysis2013" -->
	  <!--   target="_blank">Real Analysis in CS</a> program -->
	
	<tr>
	  <td><span class="duration">2008-2013</span><br/> &nbsp;</td>
	  <td>Ph.D., <a href="http://www.cs.princeton.edu"
	    target="_blank">Computer Science</a>,
	    <a href="http://www.princeton.edu"
	    target="_blank">Princeton University</a><br/>
	    Advised by <a href = "http://www.cs.princeton.edu/~arora" target="_blank">Sanjeev Arora</a> 
	  </td>
	</tr>
	
	<tr>
	  <td><span class="duration">2004-2008</span></td>
	  <td>B.Tech., <a href="http://www.cse.iitb.ac.in"
	    target="_blank">Computer Science and Engg.</a>,
	    <a href="http://www.iitb.ac.in" target="_blank">IIT Bombay</a><br/>
<!--	    B.Tech. thesis advised by
	    <a href="http://www.cse.iitb.ac.in/~sundar" target="_blank">Sundar Vishwanathan</a> -->
	  </td>
	</tr>
	
      </table>
      
    </div><!--affilation-->
    </div><!--business card-->

    <div class="section">
      <H3>Research Interests</H3>
      <p> Algorithms, and its connections to optimization, machine learning, and statistics.</p>
<!--      <p> I am broadly interested in algorithms, and its connections to optimization, machine learning, and statistics.</p>-->
<!--      <p> My research interests lie broadly in algorithms -- both towards
      designing better algorithms, and towards understanding the
      limits of efficient algorithms.  </p>-->
      <p><b>Focus areas: Design of fast algorithms, particularly for
      graph problems</b> combining techniques from convex
	optimization, and numerical linear algebra<p>
<!--	Approximation algorithms; Numerical Linear
	Algebra<p> -->
	<!--	; Hardness of approximation</p> -->
<!--      <p><b>Focus areas: Design of fast algorithms</b>, using
      techniques from convex optimization, numerical linear algebra, and
      approximation theory.<br/>
            Also, Approximation algorithms; Hardness of approximation; Analysis of
      Boolean functions.
      </p>-->
      </div><!--section-->

    <div class="section">
      <H3>Positions</H3>
      <p>The <a href="http://www.cs.toronto.edu/theory/"
      target="_blank">Theory group</a> at UToronto is looking for up
      to two Postdocs in
      TCS. <a href="http://www.cs.toronto.edu/theory/positions.html"
      target="_blank">Details</a>.  Applicants working in Algorithms
      are of particular interest to me, personally.
      </p>
      
      <p>I am seeking students with strong cs/math backgrounds
	interested in algorithms broadly. Please
      <a href="https://web.cs.toronto.edu/graduate/programs"
      target="_blank">apply directly</a> to the department, and
      indicate in your application if you're interested in working
      with me.</p>
    </div>
      


    
    <div class="section">
      <H3>Laplacian Paradigm 2.0</H3>
      <!-- <p>The <a href="http://www.cs.toronto.edu/theory/" -->
      <!-- target="_blank">Theory group</a> at UToronto is looking for up -->
      <!-- to two Postdocs in TCS, broadly -->
      <!-- construed. <a href="http://www.cs.toronto.edu/theory/positions.html" -->
      <!-- 		    target="_blank">Details</a>. -->
      <!-- </p> -->
      
      <p><a href="https://www.cc.gatech.edu/~rpeng/"
	target="_blank">Richard Peng (GaTech)</a> and I organized a
	workshop at FOCS 2018:
	 <a href="https://sachdevasushant.github.io/laplacian2.0/"
	target="_blank">Laplacian Paradigm 2.0</a></p>
    </div>

    
    <div class="section">
      <H3>Students</H3>
      <p>Deeksha Adil (Ph.D.)</p>
      </div><!--section-->

    
    <div id="pubs" class="section">
      <H3>Selected Publications</H3>






          <div class="article">
      <div class="title">
	Flows in Almost Linear Time via Adaptive Preconditioning
      </div>

      <div class="conf">
	<a href="http://acm-stoc.org/stoc2019/STOC%202019%20accepted%20papers.html" target="_blank">
	  STOC 2019
	</a>
      </div>
      
      <div class="author_list">
	with
	<span class="authors">
	  Richard Peng, Rasmus Kyng, Di Wang
	</span>
      </div>

      <div class="link_list">
	<div class="link">
	  <a href="javascript:toggle('abstract29')">Abstract</a>
	</div>
	<div class="link">
	  <a href="https://dl.acm.org/citation.cfm?id=3316410"
	     target="_blank">
	    Conference</a>
	  </div>
	<div class="link">
		    <a href="https://arxiv.org/abs/1906.10340"
		       target="_blank">arXiv</a>
	</div>
      </div>
      
      <div id="abstract29" class="abstract">
	<p>
	  We present algorithms for solving a large class of flow
	  problems on unit weighted graphs to high accuracy in
	  almost-linear time.  Our framework is inspired by the
	  routing based solver for Laplacian linear systems by
	  Spielman and Teng (STOC '04, SIMAX '14), and has a similar
	  overall recursive structure.  Our adaptation of it to flow
	  problems requires several new components including adaptive
	  non-linear preconditioning, tree routings that give
	  approximations in two norms simultaneously, and
	  gradient-preserving sparsification for wide class of
	  objective functions that interpolate between \(\ell_2\) and
	  \(\ell_{\infty}\) norms.</p>
	  
	<p> Using this framework, we obtain algorithms for computing
	  \(1 + 1/\textrm{poly}(n)\)-approximate p-norm minimizing
	  flows on unit weighted graphs in about \(2^{p^{3/2}}
	  m^{1+1/{\sqrt{p}}}\) time when \(p > 2\).  For \(p =
	  \sqrt{\log n} \) this is an almost linear time algorithm.
	  Using this algorithm as an oracle inside multiplicative
	  weights update gives: (1) a new approach for computing
	  approximate maximum flows on undirected graphs that does not
	  use oblivious routings; (2) the first almost-linear time
	  high-accuracy algorithms for $p$-norm semi-supervised
	  learning on graphs when \(p < 2\); and (3) the first
	  almost-linear time algorithm for approximate total variation
	  minimization.  </p>
					</div>
    </div><!--article ends-->


    <div class="article">
      <div class="title">
	Graph Sparsification, Spectral Sketches, and Faster Resistance
	Computation, <br/>via Short Cycle Decompositions
      </div>      
	
      <div class="conf">
	<a href="https://www.irif.fr/~focs2018/finalprogram.html"
	   target="_blank">FOCS 2018
	  </a>
      </div>

	<div class="author_list">
	with
	<span class="authors">
	  Timothy Chu, Yu Gao, Richard Peng, Saurabh Sawlani, Junxing Wang
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract25')">Abstract</a>
	</div>
	<div class="link">
	  <a href="http://ieee-focs.org/FOCS-2018-Papers/pdfs/59f361.pdf"
	  target="_blank">Conference</a>
	</div>
	<div class="link">
	  <a
	  href="https://arxiv.org/abs/1805.12051"
	  target="_blank">arXiv</a>
	</div>
	</div>
	
	<div id="abstract25" class="abstract">
	  <p>
	    We develop a framework for graph sparsification and
	    sketching, based on a new tool, short cycle decomposition
	    -- a decomposition of an unweighted graph into an
	    edge-disjoint collection of short cycles, plus few extra
	    edges. A simple observation gives that every graph \(G\)
	    on \(n\) vertices with m edges can be decomposed in
	    \(O(mn)\) time into cycles of length at most \(2 \log n\),
	    and at most \(2n\) extra edges. We give an \(m^{1+o(1)}\)
	    time algorithm for constructing a short cycle
	    decomposition, with cycles of length \(n^{o(1)}\), and
	    \(n^{1+o(1)}\) extra edges. These decompositions enable us
	    to make progress on several open questions:
	    </p>

	  <p>* We give an algorithm to find
	    \((1\pm\epsilon)\)-approximations to effective resistances
	    of all edges in time \(m^{1+o(1)}\epsilon^{-1.5}\),
	    improving over the previous best of
	    \(\tilde{O}(\min\{m\epsilon^{-2},n^2
	    \epsilon^{-1}\})\). This gives an algorithm to approximate
	    the determinant of a Laplacian up to \((1\pm\epsilon)\) in
	    \(m^{1 + o(1)} + n^{15/8+o(1)}\epsilon^{-7/4}\) time.
	  </p>

	  <p>* We show existence and efficient algorithms for
	    constructing graphical spectral sketches -- a distribution
	    over sparse graphs H such that for a fixed vector \(x\),
	    we have w.h.p. \(x'L_Hx=(1\pm\epsilon)x'L_Gx\) and
	    \(x'L_H^+x=(1\pm\epsilon)x'L_G^+x\). This implies the
	    existence of resistance-sparsifiers with about
	    \(n\epsilon^{-1}\) edges that preserve the effective
	    resistances between every pair of vertices up to
	    \((1\pm\epsilon).\)</p>

	  <p>* By combining short cycle decompositions with known
	    tools in graph sparsification, we show the existence of
	    nearly-linear sized degree-preserving spectral
	    sparsifiers, as well as significantly sparser
	    approximations of directed graphs. The latter is critical
	    to recent breakthroughs on faster algorithms for solving
	    linear systems in directed Laplacians.</p>

	  <p>Improved algorithms for constructing short cycle
	    decompositions will lead to improvements for each of the
	    above results.  </p>
	</div>
    </div><!--article ends-->

      
        <div class="article">
    <div class="title">Approximate Gaussian Elimination for Laplacians:<br/>Fast, Sparse, and Simple</div>
	
	
      <div class="conf">
	<a href="http://dimacs.rutgers.edu/FOCS16/Program/schedule.html"
	target="_blank">
	FOCS 2016
	</a>
	</div>

	<div class="author_list">
	with
	<span class="authors">
	  Rasmus Kyng
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract20')">Abstract</a>
	</div>
	<div class="link">
	  <a href="http://ieee-focs.org/FOCS-2016-Papers/3933a573.pdf"
	  target="_blank">Conference</a>
	  </div>
	<div class="link">
	  <a href="https://arxiv.org/abs/1605.02353"
	  target="_blank">arXiv</a>
	  </div>
	<div class="link">
	  <a href="https://youtu.be/oC9KQri0VPk"
	  target="_blank">Video</a>
	</div>
	</div>

	
	<div id="abstract20" class="abstract">
	  <p>
	  We show how to
	  perform sparse approximate Gaussian elimination for
	  Laplacian matrices. We present a simple, nearly linear time
	  algorithm that approximates a Laplacian by a matrix with a
	  sparse Cholesky factorization, the version of Gaussian
	  elimination for symmetric matrices. This is the first nearly
	  linear time solver for Laplacian systems that is based
	  purely on random sampling, and does not use any graph
	  theoretic constructions such as low-stretch trees,
	  sparsifiers, or expanders. The crux of our analysis is a
	  novel concentration bound for matrix martingales where the
	  differences are sums of conditionally independent variables.
	  </p>
	</div> </div><!--article ends-->

	
<!--        <div class="article">
    <div class="title">Sparsified Cholesky and Multigrid Solvers for Connection Laplacians</div>

    <div class="conf">
      <a
      href="https://www.conference-publishing.com/list.php?Event=STOC16"
      target="_blank">
      STOC 2016
      </a>
	</div>

	<div class="author_list">
	with
	<span class="authors">
	  Rasmus Kyng,
	  Yin Tat Lee,
	  Richard Peng,
	  Daniel A. Spielman
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract19')">Abstract</a>
	</div>
	<div class="link">
	  <a href="http://dl.acm.org/citation.cfm?id=2897640"
	  target="_blank">
	  Conference</a>
	</div>
	<div class="link">
	  <a href="http://arxiv.org/abs/1512.01892"
	  target="_blank">arXiv</a>
	  </div>
	</div>
	
	<div id="abstract19" class="abstract">
	  <p>
	  We introduce the
	  sparsified Cholesky and sparsified multigrid algorithms for
	  solving systems of linear equations. These algorithms
	  accelerate Gaussian elimination by sparsifying the nonzero
	  matrix entries created by the elimination process.</p>

	  <p>We use
	  these new algorithms to derive the first nearly linear time
	  algorithms for solving systems of equations in connection
	  Laplacians—a generalization of Laplacian matrices that arise
	  in many problems in image and signal processing.  </p>

	  <p>We also
	  prove that every connection Laplacian has a linear sized
	  approximate inverse. This is an LU factorization with a
	  linear number of nonzero entries that is a strong
	  approximation of the original matrix. Using such a
	  factorization one can solve systems of equations in a
	  connection Laplacian in linear time. Such a factorization
	  was unknown even for ordinary graph Laplacians.
	  </p>
	</div>
	</div>--><!--article ends-->

      
    <div class="article">
    <div class="title"> Algorithms for Lipschitz Learning on Graphs</div>
	
      <div class="conf">
	<a
	href="http://www.learningtheory.org/colt2015/the-conference/accepted-papers/"
	target="_blank">COLT 2015</a>
	</div>

	<div class="author_list">
	with
	<span class="authors">
	Rasmus Kyng,
	Anup Rao,
	Daniel Spielman
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract16')">Abstract</a>
	</div>
	<div class="link">
	  <a
	href="http://jmlr.org/proceedings/papers/v40/Kyng15.html"
	  target="_blank">Conference</a>
	</div>
	<div class="link">
	  <a href="http://arxiv.org/abs/1505.00290"
	    target="_blank">arXiv</a>
	</div>
	<div class="link">
	  <a href="http://videolectures.net/colt2015_sachdeva_lipschitz_learning/"
	    target="_blank">Video</a>
	</div>
	<div class="link">
	  Code on <a
	  href="https://github.com/danspielman/YINSlex"
	  target="_blank">Github</a>
	  </div>
	</div>
	
	<div id="abstract16" class="abstract">
	  <p>
	  We develop fast algorithms
	  for solving regression problems on graphs where one is given
	  the value of a function at some vertices, and must find its
	  smoothest possible extension to all vertices. The extension we
	  compute is the absolutely minimal Lipschitz extension, and is
	  the limit for large p of p-Laplacian regularization. We
	  present an algorithm that computes a minimal Lipschitz
	  extension in expected linear time, and an algorithm that
	  computes an absolutely minimal Lipschitz extension in expected
	  time \(\tilde{O}(mn)\). The latter algorithm has variants that
	  seem to run much faster in practice. These extensions are
	  particularly amenable to regularization: we can perform
	  \(\ell_{0}\)-regularization on the given values in polynomial
	  time and \(\ell_{1}\)-regularization on the initial function
	  values and on graph edge weights in time
	  \(\tilde{O}(m^{\frac{3}{2}})\).
	  </p>
	</div>
	</div><!--article ends-->


	<div class="article">
	<div class="title">Faster Algorithms via Approximation Theory</div>
	
	<div class="conf"><a
	  href="http://www.nowpublishers.com/articles/foundations-and-trends-in-theoretical-computer-science/TCS-065/"
	  target="_blank">FnTTCS Monograph</a>
	</div>
	
	<div class="author_list">
	  with <span class="author">Nisheeth
	  K. Vishnoi</span>
	</div>
	  
	<div class="link_list">
	<div class="link"><a
	  href="javascript:toggle('abstract13')">Abstract</a></div>
	  <div class="link"><a
	    href="http://www.nowpublishers.com/articles/foundations-and-trends-in-theoretical-computer-science/TCS-065/"
	    target="_blank">Now Publishers</a>
	  </div>
	  <div class="link"> <a href="pubs/fast-algos-via-approx-theory.pdf" target="_blank">PDF</a></div>
	  </div>
	    
	  <div id="abstract13" class="abstract">
	    <p>
	    Faster Algorithms via Approximation Theory illustrates how
	    classical and modern techniques from approximation theory play a
	    crucial role in obtaining results that are relevant to the
	    emerging theory of fast algorithms. The key lies in the fact
	    that such results imply faster ways to approximate primitives
	    such as \(A^s v, A^{-1}v, \exp(-A)v\), eigenvalues and
	    eigenvectors, which are fundamental to many spectral
	    algorithms. <br/>
	    
	    The first half of the book is devoted to the ideas and results from
	    approximation theory that are central, elegant, and may
	    have wider applicability in theoretical computer
	    science. These include not only techniques relating to
	    polynomial approximations but also those relating to
	    approximations by rational functions and beyond. The
	    remaining half illustrates a variety of ways that these
	    results can be used to design fast algorithms. <br/>

	    Faster Algorithms via Approximation Theory is
	    self-contained and should be of interest to researchers
	    and students in theoretical computer science, numerical
	    linear algebra, and related areas.
	    </p>
	  </div>
	  </div><!--article ends-->
	
	<div class="article">
	<div class="title">Provable ICA with Unknown Gaussian Noise, <br/> and Implications for Gaussian Mixtures and Autoencoders</div>

	<div class="conf">
	   <a href="http://link.springer.com/journal/453/72/1/page/1"
	  target="_blank">Algorithmica 2015</a><br/>
	  <a
	href="https://nips.cc/Conferences/2012/"
	target="_blank">NIPS 2012</a>
	  </div>
	
	<div class="author_list">
	  with <span class="authors">Sanjeev Arora, Rong Ge, Ankur
	  Moitra</span>
	</div>

	<div class="link_list">
	    <div class="link">
	      <a href="javascript:toggle('abstract7')">Abstract</a>
	    </div>
	    <div class="link">
	     <a
	href="http://link.springer.com/article/10.1007/s00453-015-9972-2"
	      target="_blank">Journal</a>
	    </div>
	    <div class="link">
	      <a
	    href="http://papers.nips.cc/paper/4603-provable-ica-with-unknown-gaussian-noise-with-implications-for-gaussian-mixtures-and-autoencoders"
	      target="_blank">Conference</a>
	    </div>
	    <div class="link">
	      <a href="http://arxiv.org/abs/1206.5349"
	      target="_blank">arXiv</a>
	    </div>
	    
	    </div>
	  
	  <div
	id="abstract7" class="abstract"> <p> We present a new algorithm
	for Independent Component Analysis (ICA) which has provable
	performance guarantees. In particular, suppose we are given
	samples of the form y = Ax + \(\eta\) where A is an unknown n
	X n matrix and x is chosen uniformly at random from \(\{+1,
	-1\}^n\), \(\eta\) is an n-dimensional Gaussian random
	variable with unknown covariance \(\Sigma\): We give an
	algorithm that provable recovers A and \(\Sigma\) up to an
	additive \(\epsilon\) whose running time and sample complexity
	are polynomial in n and \(1 / \epsilon\). To accomplish this,
	we introduce a novel "quasi-whitening" step that may be useful
	in other contexts in which the covariance of Gaussian noise is
	not known in advance. We also give a general framework for
	finding all local optima of a function (given an oracle for
	approximately finding just one) and this is a crucial step in
	our algorithm, one that has been overlooked in previous
	attempts, and allows us to control the accumulation of error
	when we find the columns of A one by one via local search.</p>
	</div>
	</div><!--article ends-->

	
    <div class="article">
    <div class="title">Approximating the Exponential, 
	the Lanczos Method and <br/> an \(\tilde{O}(m)\)-Time Spectral
	  Algorithm for Balanced Separator</div>
	
      <div class="conf">
	<a href="http://cs.nyu.edu/~stoc2012/accepted.htm" target="_blank">STOC 2012</a>
	</div>

	<div class="author_list">
	with
	<span class="authors">
	  Lorenzo Orecchia,
	  Nisheeth K. Vishnoi
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract8')">Abstract</a>
	</div>
	<div class="link">
	  <a href="http://dl.acm.org/citation.cfm?id=2214080" target="_blank">Conference</a>
	</div>
	<div class="link">
	  <a href="http://arxiv.org/abs/1111.1491"
	    target="_blank">arXiv</a>
	</div>
	<div class="link">
	  <a href="https://video.ias.edu/csdm/sachdeva"
	    target="_blank">Video</a>
	</div>
	</div>
	
	<div id="abstract8" class="abstract">
	  <p>
	   We give a novel spectral approximation algorithm for the
	  balanced separator problem that, given a graph G, a
	  constant balance b \(\in (0,\frac{1}{2}],\) and a parameter \(\gamma,\)
	  either finds an \(\Omega(b)\)-balanced cut of conductance
	  \(O(\sqrt{\gamma})\) in G, or outputs a certificate that all
	  b-balanced cuts in G have conductance at least \(\gamma,\)
	  and runs in time \(\tilde{O}(m).\) This settles the question
	  of designing asymptotically optimal spectral algorithms for
	  balanced separator.  Our algorithm relies on a variant of
	  the heat kernel random walk and requires, as a subroutine,
	  an algorithm to compute exp(-L)v where L is the
	  Laplacian of a graph related to G and v is a
	  vector. Algorithms for computing the
	  matrix-exponential-vector product efficiently comprise our
	  next set of results. Our main result here is a new algorithm
	  which computes a good approximation to exp(-A)v for a
	  class of symmetric positive semidefinite (PSD) matrices A
	  and a given vector v, in time roughly \(\tilde{O}(m_A),\)
	  where \(m_A\) is the number of non-zero entries of A. This
	  uses, in a non-trivial way, the breakthrough result of
	  Spielman and Teng on inverting symmetric and
	  diagonally-dominant matrices in \(\tilde{O}(m_A)\) time.
	  Finally, we prove that \(e^{-x}\) can be uniformly
	  approximated up to a small additive error, in a non-negative
	  interval [a,b] with a polynomial of degree roughly
	  \(\sqrt{b-a}.\) While this result is of independent interest
	  in approximation theory, we show that, via the Lanczos
	  method from numerical analysis, it yields a simple algorithm
	  to compute exp(-A)v for symmetric PSD matrices that runs
	  in time roughly \(O(t_A\cdot \sqrt{\|A\|}),\) where \(t_A\)
	  is time required for the computation of the vector Aw for
	  given vector w. As an application, we obtain a simple and
	  practical algorithm, with output conductance
	  \(O(\sqrt{\gamma}),\) for balanced separator that runs in time
	  \(\tilde{O}(\frac{m}{\sqrt{\gamma}}).\) This latter algorithm
	  matches the running time, but improves on the approximation
	  guarantee of the Evolving-Sets-based algorithm by Andersen
	  and Peres for balanced separator.</p>
	</div>
	</div><!--article ends-->


<!--	     	<div class="article">
	<div class="title">Inapproximability of Minimum Vertex Cover
	  on <br/>
	  k-Uniform k-Partite Hypergraphs</div>
	
	<div class="conf">
	  <a
	href="http://epubs.siam.org/toc/sjdmec/29/1"
	target="_blank">SIDMA 2015</a>
	</div>

	<div class="author_list">
	with
	  <span class="authors">
	  Venkatesan Guruswami,
	  Rishi Saket
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract14')">Abstract</a>
	</div>
	<div class="link">
	  <a href="http://epubs.siam.org/doi/abs/10.1137/130919416"
	  target="_blank">Journal</a>
	</div>
	<div class="link">
	  <a href="http://eccc.hpi-web.de/report/2013/071/" target="_blank">ECCC</a>
	  </div>
	</div>
	
	<div id="abstract14" class="abstract">
	  <p>
	  	  We study the problem of computing the minimum vertex cover
	  on k-uniform k-partite hypergraphs when the
	  k-partition is given. On bipartite graphs (k=2), the
	  minimum vertex cover can be computed in polynomial time. For
	  k\(\ge 3,\) this problem is known to be NP-hard. For general
	  k, the problem was studied by Lovász, who gave a
	  \(\frac{k}{2}\)-approximation based on the standard LP
	  relaxation. Subsequent work by Aharoni, Holzman, and
	  Krivelevich showed a tight integrality gap of \(\frac{k}{2}
	  - o(1))\) for the LP relaxation. We further investigate the
	  inapproximability of minimum vertex cover on k-uniform
	  k-partite hypergraphs and present the following results
	  (here \(\varepsilon  > 0\) is an arbitrarily small constant):
	  NP-hardness of obtaining an approximation factor of
	  \((\frac{k}{4} - \varepsilon)\) for even k and \((\frac{k}{4}
	  - \frac{1}{4k} - \varepsilon)\) for odd k, NP-hardness of
	  obtaining a nearly optimal approximation factor of
	  \((\frac{k}{2}-1+\frac{1}{2k}-\varepsilon)\), and an optimal
	  unique games-hardness for approximation within factor
	  \((\frac{k}{2} - \varepsilon)\), showing the optimality of
	  Lovász's algorithm if one assumes the Unique Games
	  conjecture. The first hardness result is based on a
	  reduction from minimum vertex cover in r-uniform
	  hypergraphs, for which NP-hardness of approximating within
	  \(r - 1 -\varepsilon\) was shown by Dinur, Guruswami, Khot,
	  and Regev. We include it for its simplicity, despite it
	  being subsumed by the second hardness result. The unique
	  games-hardness result is obtained by applying the results of
	  Kumar, Manokaran, Tulsiani, and Vishnoi, with a slight
	  modification, to the LP integrality gap due to Aharoni,
	  Holzman, and Krivelevich. The modification ensures that the
	  reduction preserves the desired structural properties of the
	  hypergraph. The reduction for the nearly optimal NP-hardness
	  result relies on the multilayered PCP of Dinur, Guruswami,
	  Khot, and Regev and uses a gadget based on biased long codes
	  adapted from the LP integrality gap of Aharoni, Holzman, and
	  Krivelevich. Our reduction requires the analysis of several
	  long codes with different biases, for which we prove
	  structural properties of the so-called cross-intersecting
	  collections of set families, variants of which have been
	  studied in extremal set theory
	  </p>
	</div>
	</div>--><!--article ends-->

	
      </div><!--pubs end-->

      <div class="section">
      </div>
</div> <!--content end-->
</div> <!--container end-->
</body>
</html>
