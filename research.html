<!doctype html>
<html lang="en">

<head>
<meta charset="UTF-8">
<title>Sushant Sachdeva</title>
<meta name="description" content="Sushant Sachdeva's homepage">
<meta name="keywords" content="research,yale,sushant,sachdeva,theory,cs">
<meta name="author" content="Sushant Sachdeva">
<!--link rel="stylesheet" href="reset.css" type="text/css" media="screen"-->
<link rel="stylesheet" href="newstyle.css" type="text/css" media="screen" />
</head>

<body>


<!--Script for Google Analytics-->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-3519710-6', 'auto');
  ga('send', 'pageview');

</script>
<!-- End script for Google Analytics-->

<!--Script for toggling abstracts-->
<SCRIPT  type="text/javascript"> 
  function toggle(aa) {
    state=document.getElementById(aa).style.display
    if (state == '' || state == 'none') {
	  document.getElementById(aa).style.display='block';
    }
    if (state == 'block') {
	  document.getElementById(aa).style.display='none';
    }
  }
</SCRIPT>
<!--End Script for toggling abstracts-->

<!--Script for mathjax-->
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!--End Script for toggling abstracts-->




<div id="container">
  <header>

  
<div id="nav">
  <div id="mainmenu">
  <div id="banner">
    <h1><a href="index.html">Sushant Sachdeva</a></h1>
  </div><!--end banner-->
  <div id="menu">
  <ul>
    <li><a href="contact.html" >Contact</a></li>
    <li><a href="teaching.html" >Teaching</a></li>
    <li><a href="research.html" ><div class="active_page">Research</div></a></li>
    <li><a href="app/cv-sachdeva.pdf" target="_blank">CV</a></li>
  </ul>
</div><!--menu end-->
</div><!--mainmenu end-->
<div id="submenu">
  <ul style="padding-right:8.445em">
    <li><a href="pubs.html" >By Year</a></li>
    <li><a href="research.html" ><div class="active_page">By Topic</div></a></li>
    </ul>
  </div><!--submenu end-->
</div><!--nav end-->
</header>


  <div id="content">
<!--<div id="announcement">
    I am looking for an academic position starting Fall 2016.
    </div>-->
  <div id="pubs" class="section">
    <H3>Design of Fast Algorithms</H3>


    <div class="article">
      <div class="title">
	Iterative Refinement for \(\ell_p\)-norm Regression
      </div>      

      <div class="conf">
	<a href="https://www.siam.org/conferences/CM/P/AP/soda19-accepted-papers"
	   target="_blank">SODA 2019</a>
      </div>

	<div class="author_list">
	with
	<span class="authors">
	  Deeksha Adil, Rasmus Kyng, Richard Peng
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract27')">Abstract</a>
	</div> 
<!--	<div class="link">
	  <a
	  href="https://arxiv.org/abs/1810.05143"
	  target="_blank">arXiv</a>
	</div> -->
	</div> 
	
	<div id="abstract27" class="abstract">
	  <p>
	    We give improved algorithms for the \(\ell_p\)-regression
	    problem, \(\min \|x\|_p\) such that \(Ax = b\), for all
	    \(p \in (1,2) \cup (2,\infty)\). Our algorithms obtain a
	    high accuracy solution in
	    \(\tilde{O}_p(m^{\frac{|p-2|}{2p+|p-2|}}) \le
	    \tilde{O}_p(m^{{1}/{3}})\) iterations, where each
	    iteration requires solving an \(m \times m\) linear
	    system, with \(m\) being the dimension of the ambient
	    space.</p>
	  <p>Incorporating a procedure for maintaining an
	    approximate inverse of the linear systems that we need to
	    solve at each iteration, we give algorithms for solving
	    \(\ell_p\)-regression to 1/poly(n) accuracy that runs in
	    time \(\tilde{O}_p(m^{\max\{\omega, 7/3\}}) \), where
	    \(\omega\) is the matrix multiplication constant. For the
	    current best value of \(\omega > 2.37\), this means that
	    we can solve \(\ell_p\) regression as fast as \(\ell_p\)
	    regression, for all constant \(p\) bounded away from
	    1.</p>
	  <p>
	    Our algorithms can be combined with nearly-linear time
	    solvers for linear systems in graph Laplacians to give
	    minimum \(\ell_p\)-norm flow / voltage solutions to
	    1/poly(n) accuracy on an undirected graph with \(m\) edges
	    in \(\tilde{O}_p(m^{1+\frac{|p-2|}{2p+|p-2|}}) \le
	    \tilde{O}_p(m^{{4}/{3}})\) time. </p>
	  <p>For sparse graphs and for
	    matrices with similar dimensions, our iteration counts and
	    running times improve upon the p-norm regression algorithm
	    by [Bubeck-Cohen-Lee-Li STOC‘18], as well as general
	    purpose convex optimization algorithms. At the core of our
	    algorithms is an iterative refinement scheme for
	    \(\ell_p\)-norms, using the quadratically-smoothed
	    \(\ell_p\)-norms introduced in the work of Bubeck et
	    al. Formally, given an initial solution, we construct a
	    problem that seeks to minimize a quadratically-smoothed
	    \(\ell_p\) norm over a subspace, such that a crude
	    solution to this problem allows us to improve the initial
	    solution by a constant factor, leading to algorithms with
	    fast convergence.
	  </p>

	</div>
    </div><!--article ends-->


    

    <div class="article">
      <div class="title">
	Short Cycles via Low-Diameter Decompositions
      </div>      
	
      <div class="conf">
	<a href="https://www.siam.org/conferences/CM/P/AP/soda19-accepted-papers"
	   target="_blank">SODA 2019</a>
      </div>

	<div class="author_list">
	with
	<span class="authors">
	  Yang P. Liu, Zejun Yu
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract26')">Abstract</a>
	</div>
	<div class="link">
	  <a
	  href="https://arxiv.org/abs/1810.05143"
	  target="_blank">arXiv</a>
	</div>
	</div>
	
	<div id="abstract26" class="abstract">
	  <p>
	    We present improved algorithms for short cycle
	    decomposition of a graph. Short cycle decompositions were
	    introduced in the recent work of Chu et al, and were used
	    to make progress on several questions in graph
	    sparsification.
	  </p>
	  <p>
	    For all constants \(\delta \in (0,1]\), we give an
\(O(mn^\delta)\) time algorithm that, given a graph G, partitions its
edges into cycles of length \(O(\log n)^{\frac{1}{\delta}}\), with
\(O(n)\) extra edges not in any cycle. This gives the first
subquadratic, in fact almost linear time, algorithm achieving
polylogarithmic cycle lengths. We also give an \(m\exp(O(\sqrt{\log
n}))\) time algorithm that partitions the edges of a graph into cycles
of length \(exp(O(\sqrt{\log n}\log \log n))\), with \(O(n)\) extra
edges not in any cycle. This improves on the short cycle decomposition
algorithms given in Chu et al in terms of all parameters, and is
significantly simpler.
	  </p>
	  <p>As a result, we obtain faster algorithms and improved
guarantees for several problems in graph sparsification --
construction of resistance sparsifiers, graphical spectral sketches,
degree preserving sparsifiers, and approximating the effective
resistances of all edges.
	    	    </p>

	</div>
    </div><!--article ends-->








    <div class="article">
      <div class="title">
	Graph Sparsification, Spectral Sketches, and Faster Resistance
	Computation, <br/>via Short Cycle Decompositions
      </div>      
	
      <div class="conf">
	<a href="https://www.irif.fr/~focs2018/finalprogram.html"
	   target="_blank"> FOCS 2018
	  </a>
      </div>

	<div class="author_list">
	with
	<span class="authors">
	  Timothy Chu, Yu Gao, Richard Peng, Saurabh Sawlani, Junxing Wang
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract25')">Abstract</a>
	</div>
	<div class="link">
	  <a href="http://ieee-focs.org/FOCS-2018-Papers/pdfs/59f361.pdf"
	  target="_blank">Conference</a>
	</div>

	<div class="link">
	  <a
	  href="https://arxiv.org/abs/1805.12051"
	  target="_blank">arXiv</a>
	</div>
	</div>
	
	<div id="abstract25" class="abstract">
	  <p>
	    We develop a framework for graph sparsification and
	    sketching, based on a new tool, short cycle decomposition
	    -- a decomposition of an unweighted graph into an
	    edge-disjoint collection of short cycles, plus few extra
	    edges. A simple observation gives that every graph \(G\)
	    on \(n\) vertices with m edges can be decomposed in
	    \(O(mn)\) time into cycles of length at most \(2 \log n\),
	    and at most \(2n\) extra edges. We give an \(m^{1+o(1)}\)
	    time algorithm for constructing a short cycle
	    decomposition, with cycles of length \(n^{o(1)}\), and
	    \(n^{1+o(1)}\) extra edges. These decompositions enable us
	    to make progress on several open questions:
	    </p>

	  <p>* We give an algorithm to find
	    \((1\pm\epsilon)\)-approximations to effective resistances
	    of all edges in time \(m^{1+o(1)}\epsilon^{-1.5}\),
	    improving over the previous best of
	    \(\tilde{O}(\min\{m\epsilon^{-2},n^2
	    \epsilon^{-1}\})\). This gives an algorithm to approximate
	    the determinant of a Laplacian up to \((1\pm\epsilon)\) in
	    \(m^{1 + o(1)} + n^{15/8+o(1)}\epsilon^{-7/4}\) time.
	  </p>

	  <p>* We show existence and efficient algorithms for
	    constructing graphical spectral sketches -- a distribution
	    over sparse graphs H such that for a fixed vector \(x\),
	    we have w.h.p. \(x'L_Hx=(1\pm\epsilon)x'L_Gx\) and
	    \(x'L_H^+x=(1\pm\epsilon)x'L_G^+x\). This implies the
	    existence of resistance-sparsifiers with about
	    \(n\epsilon^{-1}\) edges that preserve the effective
	    resistances between every pair of vertices up to
	    \((1\pm\epsilon).\)</p>

	  <p>* By combining short cycle decompositions with known
	    tools in graph sparsification, we show the existence of
	    nearly-linear sized degree-preserving spectral
	    sparsifiers, as well as significantly sparser
	    approximations of directed graphs. The latter is critical
	    to recent breakthroughs on faster algorithms for solving
	    linear systems in directed Laplacians.</p>

	  <p>Improved algorithms for constructing short cycle
	    decompositions will lead to improvements for each of the
	    above results.  </p>
	</div>
    </div><!--article ends-->





















    
            <div class="article">
    <div class="title">Sampling Random Spanning Trees Faster than Matrix
	Multiplication</div>
	
	
      <div class="conf">
	<a href="http://acm-stoc.org/stoc2017/STOC2017accepted.pdf"
	target="_blank">
	STOC 2017
	</a>
	</div>

	<div class="author_list">
	with
	<span class="authors">
	  David Durfee, Rasmus Kyng, John Peebles, Anup B. Rao
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract22')">Abstract</a>
	</div>
	<div class="link">
	  <a
	  href="http://dl.acm.org/citation.cfm?id=3055499"
	  target="_blank">Conference</a>
	</div>
	<div class="link">
	  <a
	  href="https://arxiv.org/abs/1611.07451"
	  target="_blank">arXiv</a>
	</div>
	</div>
	
	<div id="abstract22" class="abstract"> <p> We present an
	    algorithm that, with high probability, generates a random
	    spanning tree from an edge-weighted undirected graph in
	    \(\tilde{O}(n^{\frac{4}{3}} m^{\frac{1}{2}} +
	    n^2)\) time (The \(\tilde{O}\) notation hides polylog(n)
	    factors). The tree is sampled from a distribution where
	    the probability of each tree is proportional to the
	    product of its edge weights. This improves upon the
	    previous best algorithm due to Colbourn et al. that runs
	    in matrix multiplication time, \(O(n^\omega)\). For the
	    special case of unweighted graphs, this improves upon the
	    best previously known running time of
	    \(\tilde{O}(\min\{n^\omega, m\sqrt{n},
	    m^{\frac{4}{3}}\})\) for \(m \gg n^{\frac{5}{3}}\)
	    (Colbourn et al. '96, Kelner-Madry '09, Madry et al. '15).</p>
	    
	    <p>The effective resistance metric is essential to our
algorithm, as in the work of Madry et al., but we eschew
determinant-based and random walk-based techniques used by previous
algorithms. Instead, our algorithm is based on Gaussian elimination,
and the fact that effective resistance is preserved in the graph
resulting from eliminating a subset of vertices (called a Schur
complement). As part of our algorithm, we show how to compute
ϵ-approximate effective resistances for a set S of vertex pairs via
approximate Schur complements in \(\tilde{O}(m +
(n+|S|)\epsilon^{-2})\) time, without using the Johnson-Lindenstrauss
lemma which requires \(\tilde{O} \min\{(m+|S|)\epsilon^{-2}, m +n
\epsilon^{-4} + |S| \epsilon^{-2}\} \)  time. We combine this approximation
procedure with an error correction procedure for handing edges where
our estimate isn't sufficiently accurate.
	  </p>
	</div> </div><!--article ends-->

    
              <div class="article">
    <div class="title">A Framework for Analyzing Resparsification Algorithms</div>
	
	
      <div class="conf">
	<a href="https://www.siam.org/meetings/da17/da17_accepted.pdf"
	target="_blank">
	SODA 2017
	</a>
	</div>

	<div class="author_list">
	with
	<span class="authors">
	  Rasmus Kyng, Jakub Pachocki, Richard Peng
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract21')">Abstract</a>
	</div>
	<div class="link">
	  <a
	  href="https://dl.acm.org/citation.cfm?id=3039818"
	  target="_blank">Conference</a>
	</div>
	<div class="link">
	  <a href="https://arxiv.org/abs/1611.06940"
	  target="_blank">arXiv</a>
	</div>

	</div>
	
	<div id="abstract21" class="abstract"> <p> A spectral
	  sparsifier of a graph G is a sparser graph H that
	  approximately preserves the quadratic form of G, i.e., for
	  all vectors x, \(x^{\top}L_G x \approx x^{\top}L_H x\) ,
	  where \(L_G, L_H\) denote the respective graph
	  Laplacians. Spectral sparsifiers generalize cut sparsifiers,
	  and have found several applications in designing graph
	  algorithms. In recent years, there has been interest in
	  computing spectral sparsifiers in the semi-streaming and
	  dynamic settings. Natural algorithms in these settings
	  involve repeated sparsification of a dynamic graph. We
	  present a framework for analyzing algorithms for graph
	  sparsification that perform repeated sparsifications. The
	  framework yields analyses that avoid a worst-case error
	  accumulation across various resparsification steps, and only
	  incur the error corresponding to a single sparsification
	  step leading to better results.
	  
As an application, we show how to maintain a spectral sparsifier of a
	  graph, with \(O(n \log n)\) edges in a semi-streaming setting: We
	  present a simple algorithm that, for a graph G on n
	  vertices and m edges, computes a spectral sparsifier of G
	  with \(O(n \log n)\) edges in a single pass over G, using only
	  \(O(n \log n)\) space, and \(O(m \log^2 n)\) total time. This improves
	  on previous best constructions in the semi-streaming set-
	  ting for both spectral and cut sparsifiers by a factor of
	  log n in both space and runtime. The
	  algorithm also extends to semi-streaming row sampling for
	  general PSD matrices. As another example, we use this
	  framework to combining an algorithm due to Koutis with
	  improved spanner constructions to give a parallel
	  algorithm for construction \(O(n \log^2 n \log \log n)\) sized
	  spectral sparsifiers in \(O(m \log^2 n \log \log n)\) time. This is
	  the best combinatorial graph sparsification algorithm to
	  date, and the size of the sparsifiers produced is only a
	  factor \(\log n \log \log n\) more than numerical ones.

	  </p>
	</div> </div><!--article ends-->
	


    	        <div class="article">
		<div class="title">
		  The mixing time of the Dikin walk in a polytope—A simple proof</div>
	
      <div class="conf">
	<a href="http://www.sciencedirect.com/science/journal/01676377" target="_blank">OR Letters 2016</a>
	</div>

	<div class="author_list">
	with
	<span class="authors">
	  Nisheeth K. Vishnoi
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract18')">Abstract</a>
	</div>
	<div class="link">
	  <a
	    href="http://www.sciencedirect.com/science/article/pii/S0167637716300621"
	    target="_blank">Journal</a> </div>
	<div class="link">
	  <a
	  href="http://arxiv.org/abs/1508.01977"
	  target="_blank">arXiv</a>
	</div>
	</div>
	
	<div id="abstract18" class="abstract">
	  <p>
	  Sampling points from the uniform distribution on a polytope
	  is a well-studied problem, and is an important ingredient in
	  several computational tasks involving polytopes, such as
	  volume estimation. This is achieved by setting up a random
	  walk inside the polytope, with its stationary distribution
	  being uniform in the interior of the
	  polytope. Kannan-Narayanan and Narayanan proposed the Dikin
	  walk based on interior point methods, where the next point
	  is sampled, roughly, from the Dikin ellipsoid at the current
	  point. In this paper, we give a simple proof of the mixing
	  time of the Dikin walk, using well-known properties of
	  Gaussians, and concentration of Gaussian polynomials.
	  </p>
	</div>
	</div><!--article ends-->


     
        <div class="article">
    <div class="title">Approximate Gaussian Elimination for Laplacians:<br/>Fast, Sparse, and Simple</div>
	
	
      <div class="conf">
	<a href="http://dimacs.rutgers.edu/FOCS16/Program/schedule.html"
	target="_blank">
	FOCS 2016
	</a>
	</div>

	<div class="author_list">
	with
	<span class="authors">
	  Rasmus Kyng
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract20')">Abstract</a>
	</div>
	<div class="link">
	  <a href="http://ieee-focs.org/FOCS-2016-Papers/3933a573.pdf"
	  target="_blank">Conference</a>
	  </div>
	<div class="link">
	  <a href="https://arxiv.org/abs/1605.02353"
	  target="_blank">arXiv</a>
	  </div>
	<div class="link">
	  <a href="https://youtu.be/oC9KQri0VPk"
	  target="_blank">Video</a>
	</div>
	</div>
	
	<div id="abstract20" class="abstract">
	  <p>
	  We show how to
	  perform sparse approximate Gaussian elimination for
	  Laplacian matrices. We present a simple, nearly linear time
	  algorithm that approximates a Laplacian by a matrix with a
	  sparse Cholesky factorization, the version of Gaussian
	  elimination for symmetric matrices. This is the first nearly
	  linear time solver for Laplacian systems that is based
	  purely on random sampling, and does not use any graph
	  theoretic constructions such as low-stretch trees,
	  sparsifiers, or expanders. The crux of our analysis is a
	  novel concentration bound for matrix martingales where the
	  differences are sums of conditionally independent variables.
	  </p>
	</div> </div><!--article ends-->
	
        <div class="article">
    <div class="title">Sparsified Cholesky and Multigrid Solvers for Connection Laplacians</div>
	
    <div class="conf">
            <a
      href="https://www.conference-publishing.com/list.php?Event=STOC16"
      target="_blank">
      STOC 2016
      </a>
	</div>

	<div class="author_list">
	with
	<span class="authors">
	  Rasmus Kyng,
	  Yin Tat Lee,
	  Richard Peng,
	  Daniel A. Spielman
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract19')">Abstract</a>
	</div>
	<div class="link">
	  <a href="http://dl.acm.org/citation.cfm?id=2897640"
	  target="_blank">
	  Conference</a>
	</div>
	<div class="link">
	  <a href="http://arxiv.org/abs/1512.01892"
	  target="_blank">arXiv</a>
	  </div>
	</div>
	
	<div id="abstract19" class="abstract">
	  <p>
	  We introduce the
	  sparsified Cholesky and sparsified multigrid algorithms for
	  solving systems of linear equations. These algorithms
	  accelerate Gaussian elimination by sparsifying the nonzero
	  matrix entries created by the elimination process.</p>

	  <p>We use
	  these new algorithms to derive the first nearly linear time
	  algorithms for solving systems of equations in connection
	  Laplacians—a generalization of Laplacian matrices that arise
	  in many problems in image and signal processing.  </p>

	  <p>We also
	  prove that every connection Laplacian has a linear sized
	  approximate inverse. This is an LU factorization with a
	  linear number of nonzero entries that is a strong
	  approximation of the original matrix. Using such a
	  factorization one can solve systems of equations in a
	  connection Laplacian in linear time. Such a factorization
	  was unknown even for ordinary graph Laplacians.
	  </p>
	</div>
	</div><!--article ends-->
	




		<div class="article">
	<div class="title">Faster Algorithms via Approximation Theory</a></div>
	
	<div class="conf"><a
	  href="http://www.nowpublishers.com/articles/foundations-and-trends-in-theoretical-computer-science/TCS-065/"
	  target="_blank">FnTTCS Monograph</a>
	</div>
	
	<div class="author_list">
	  with <span class="author">Nisheeth
	  K. Vishnoi</span>
	</div>
	  
	<div class="link_list">
	<div class="link"><a
	  href="javascript:toggle('abstract13')">Abstract</a></div>
	  <div class="link"><a
	    href="http://www.nowpublishers.com/articles/foundations-and-trends-in-theoretical-computer-science/TCS-065/"
	    target="_blank">Now Publishers</a>
	  </div>
	  <div class="link"> <a href="pubs/fast-algos-via-approx-theory.pdf" target="_blank">PDF</a></div>
	  </div>
	    
	  <div id="abstract13" class="abstract">
	    <p>
	    Faster Algorithms via Approximation Theory illustrates how
	    classical and modern techniques from approximation theory play a
	    crucial role in obtaining results that are relevant to the
	    emerging theory of fast algorithms. The key lies in the fact
	    that such results imply faster ways to approximate primitives
	    such as \(A^s v, A^{-1}v, \exp(-A)v\), eigenvalues and
	    eigenvectors, which are fundamental to many spectral
	    algorithms. </p>
	    
	    <p>The first half of the book is devoted to the ideas and results from
	    approximation theory that are central, elegant, and may
	    have wider applicability in theoretical computer
	    science. These include not only techniques relating to
	    polynomial approximations but also those relating to
	    approximations by rational functions and beyond. The
	    remaining half illustrates a variety of ways that these
	    results can be used to design fast algorithms. </p>

	    <p>Faster Algorithms via Approximation Theory is
	    self-contained and should be of interest to researchers
	    and students in theoretical computer science, numerical
	    linear algebra, and related areas.
	    </p>
	  </div>
	  </div><!--article ends-->


	
	
	

	
		    <div class="article">
    <div class="title">Approximating the Exponential, 
	the Lanczos Method and <br/> an \(\tilde{O}(m)\)-Time Spectral
	  Algorithm for Balanced Separator</div>
	
      <div class="conf">
	<a href="http://cs.nyu.edu/~stoc2012/accepted.htm" target="_blank">STOC 2012</a>
	</div>

	<div class="author_list">
	with
	<span class="authors">
	  Lorenzo Orecchia,
	  Nisheeth K. Vishnoi
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract8')">Abstract</a>
	</div>
	<div class="link">
	  <a href="http://dl.acm.org/citation.cfm?id=2214080" target="_blank">Conference</a>
	</div>
	<div class="link">
	  <a href="http://arxiv.org/abs/1111.1491"
	    target="_blank">arXiv</a>
	</div>
	<div class="link">
	  <a href="https://video.ias.edu/csdm/sachdeva"
	    target="_blank">Video</a>
	</div>
	</div>
	
	<div id="abstract8" class="abstract">
	  <p>
	   We give a novel spectral approximation algorithm for the
	  balanced separator problem that, given a graph G, a
	  constant balance b \(\in (0,\frac{1}{2}],\) and a parameter \(\gamma,\)
	  either finds an \(\Omega(b)\)-balanced cut of conductance
	  \(O(\sqrt{\gamma})\) in G, or outputs a certificate that all
	  b-balanced cuts in G have conductance at least \(\gamma,\)
	  and runs in time \(\tilde{O}(m).\) This settles the question
	  of designing asymptotically optimal spectral algorithms for
	  balanced separator.  Our algorithm relies on a variant of
	  the heat kernel random walk and requires, as a subroutine,
	  an algorithm to compute exp(-L)v where L is the
	  Laplacian of a graph related to G and v is a
	  vector. Algorithms for computing the
	  matrix-exponential-vector product efficiently comprise our
	  next set of results. Our main result here is a new algorithm
	  which computes a good approximation to exp(-A)v for a
	  class of symmetric positive semidefinite (PSD) matrices A
	  and a given vector v, in time roughly \(\tilde{O}(m_A),\)
	  where \(m_A\) is the number of non-zero entries of A. This
	  uses, in a non-trivial way, the breakthrough result of
	  Spielman and Teng on inverting symmetric and
	  diagonally-dominant matrices in \(\tilde{O}(m_A)\) time.
	  Finally, we prove that \(e^{-x}\) can be uniformly
	  approximated up to a small additive error, in a non-negative
	  interval [a,b] with a polynomial of degree roughly
	  \(\sqrt{b-a}.\) While this result is of independent interest
	  in approximation theory, we show that, via the Lanczos
	  method from numerical analysis, it yields a simple algorithm
	  to compute exp(-A)v for symmetric PSD matrices that runs
	  in time roughly \(O(t_A\cdot \sqrt{\|A\|}),\) where \(t_A\)
	  is time required for the computation of the vector Aw for
	  given vector w. As an application, we obtain a simple and
	  practical algorithm, with output conductance
	  \(O(\sqrt{\gamma}),\) for balanced separator that runs in time
	  \(\tilde{O}(\frac{m}{\sqrt{\gamma}}).\) This latter algorithm
	  matches the running time, but improves on the approximation
	  guarantee of the Evolving-Sets-based algorithm by Andersen
	  and Peres for balanced separator.</p>
	</div>
	</div><!--article ends-->

	
	
	
	
<!--	</div>


  <div id="pubs" class="section">
    <H3>Using Approximation Theory for Fast Algorithms</H3> -->
    


	<div class="article">
	<div class="title">Matrix Inversion is as easy as Exponentiation</div>
	
	<div class="conf">
	  Manuscript
	</div>

	<div class="author_list">
	with
	  <span class="authors">
	  Nisheeth K. Vishnoi
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract9')">Abstract</a>
	</div>
	<div class="link">
	  	<a href="http://arxiv.org/abs/1305.0526"
	  target="_blank">arXiv</a>
	</div>
	</div>
	
	<div id="abstract9" class="abstract">
	  <p>
	  We prove that the inverse of a positive-definite matrix can
	  be approximated by a weighted-sum of a small number of matrix
	  exponentials. Combining this with a previous result [OSV12], we
	  establish an equivalence between matrix inversion and exponentiation
	  up to polylogarithmic factors. In particular, this connection
	  justifies the use of Laplacian solvers for designing fast
	  semi-definite programming based algorithms for certain graph
	  problems. The proof relies on the Euler-Maclaurin formula and certain
	  bounds derived from the Riemann zeta function.
	  </p>
	</div>
	</div><!--article ends-->

	
  </div>
    
  <div id="pubs" class="section">
    <H3>Algorithmic Questions in Learning and Statistics</H3>



    <div class="article">
      <div class="title">
	Multi-component Graph based Semi-Supervised Learning
      </div>      

      <div class="conf">AISTATS 2019
      </div>

	<div class="author_list">
	with
	<span class="authors">
	  Krishnamurthy Viswanathan, Andrew Tomkins, Sujith Ravi
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract28')">Abstract</a>
	</div> 
<!--	<div class="link">
	  <a
	  href="https://arxiv.org/abs/1810.05143"
	  target="_blank">arXiv</a>
	</div> -->
	</div> 
	
	<div id="abstract28" class="abstract">
	  <p>
	  </p>
We present a new approach for graph based semi-supervised learning based on a multi- component extension to the Gaussian MRF model. This approach models the observa- tions on the vertices as a Gaussian ensemble with an inverse covariance matrix that is a weighted linear combination of multiple ma- trices. Building on randomized matrix trace estimation and fast Laplacian solvers, we de- velop fast and efficient algorithms for comput- ing the best-fit (maximum likelihood) model and the predicted labels using gradient de- scent. Our model is considerably simpler, with just tens of parameters, and a single hyperparameter, in contrast with state-of-the- art approaches using deep learning techniques. Our experiments on benchmark citation net- works show that the best-fit model estimated by our algorithm leads to significant improve- ments on all datasets compared to baseline models. Further, our performance compares favorably with several state-of-the-art meth- ods on these datasets, and is comparable with the best performances.
	</div>
    </div><!--article ends-->



    
        <div class="article">
    <div class="title">Convergence Results for Neural Networks via
    Electrodynamics</div>
	
	
      <div class="conf">
	<a href="http://itcs-conf.org/itcs18/itcs18-accepted.html"
	target="_blank"> ITCS 2018
	</a>
	</div>

	<div class="author_list">
	with
	<span class="authors">
	  Rina Panigrahy, Ali Rahimi, Qiuyi Zhang
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract23')">Abstract</a>
	</div>
	<div class="link">
	  <a
	  href="http://vesta.informatik.rwth-aachen.de/opus/volltexte/2018/8352/pdf/LIPIcs-ITCS-2018-22.pdf"
	  target="_blank">Conference</a>
	</div>
	<div class="link">
	  <a
	  href="https://arxiv.org/abs/1702.00458"
	  target="_blank">arXiv</a>
	</div>
	</div>
	
	<div id="abstract23" class="abstract"> <p> We study whether a
	    depth two neural network can learn another depth two
	    network using gradient descent. Assuming a linear output
	    node, we show that the question of whether gradient
	    descent converges to the target function is equivalent to
	    the following question in electrodynamics: Given k fixed
	    protons in \(\mathbb{R}^d\), and k electrons, each moving due to the
	    attractive force from the protons and repulsive force from
	    the remaining electrons, whether at equilibrium all the
	    electrons will be matched up with the protons, up to a
	    permutation. Under the standard electrical force, this
	    follows from the classic Earnshaw's theorem. In our
	    setting, the force is determined by the activation
	    function and the input distribution. Building on this
	    equivalence, we prove the existence of an activation
	    function such that gradient descent learns at least one of
	    the hidden nodes in the target network. Iterating, we show
	    that gradient descent can be used to learn the entire
	    network one node at a time.
	  </p>
	</div> </div><!--article ends-->


    	<div class="article">
	<div class="title">Fast, Provable
	Algorithms for Isotonic Regression in all \(\ell_{p}\)-norms</div>
	
	<div class="conf">
	  <a href="https://nips.cc/Conferences/2015/AcceptedPapers" target="_blank">NIPS 2015</a>
	</div>

	<div class="author_list">
	with
	  <span class="authors">
	  Rasmus Kyng, Anup Rao
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract17')">Abstract</a>
	</div>
	<div class="link">
	  <a
	  href="http://arxiv.org/abs/1507.00710"
	  target="_blank">arXiv</a>
	</div>
	<div class="link">
	  Code on
	  <a href="https://github.com/sachdevasushant/Isotonic/"
	  target="_blank">Github</a>
	</div>
	</div>
	
	<div id="abstract17" class="abstract">
	  <p>
	  Given a directed acyclic graph G, and a set of values y on
	the vertices, the Isotonic Regression of y is a vector x that
	respects the partial order described by G, and minimizes
	\(\|x−y\|\), for a specified norm. This paper gives improved
	algorithms for computing the Isotonic Regression for all
	weighted \(\ell_{p}\)-norms with rigorous performance guarantees. Our
	algorithms are quite practical, and their variants can be
	  implemented to run fast in practice.
	  </p>
	</div>
	</div><!--article ends-->


	

	<div class="article">
	  <div class="title"> Algorithms for Lipschitz Learning on
	    Graphs</div>
	  
      <div class="conf">
	<a href="http://www.learningtheory.org/colt2015/the-conference/accepted-papers/"
	target="_blank">COLT 2015</a>
	</div>

	<div class="author_list">
	with
	<span class="authors">
	Rasmus Kyng,
	Anup Rao,
	Daniel Spielman
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract16')">Abstract</a>
	</div>
	<div class="link">
	  <a
	href="http://jmlr.org/proceedings/papers/v40/Kyng15.html"
	  target="_blank">Conference</a>
	</div>
	<div class="link">
	  <a href="http://arxiv.org/abs/1505.00290"
	    target="_blank">arXiv</a>
	</div>
	<div class="link">
	  <a href="http://videolectures.net/colt2015_sachdeva_lipschitz_learning/"
	    target="_blank">Video</a>
	</div>
	<div class="link">
	  Code on <a
	  href="https://github.com/danspielman/YINSlex"
	  target="_blank">Github</a>
	  </div>
	</div>
	
	<div id="abstract16" class="abstract">
	  <p>
	  We develop fast algorithms
	  for solving regression problems on graphs where one is given
	  the value of a function at some vertices, and must find its
	  smoothest possible extension to all vertices. The extension we
	  compute is the absolutely minimal Lipschitz extension, and is
	  the limit for large p of p-Laplacian regularization. We
	  present an algorithm that computes a minimal Lipschitz
	  extension in expected linear time, and an algorithm that
	  computes an absolutely minimal Lipschitz extension in expected
	  time \(\tilde{O}(mn)\). The latter algorithm has variants that
	  seem to run much faster in practice. These extensions are
	  particularly amenable to regularization: we can perform
	  \(\ell_{0}\)-regularization on the given values in polynomial
	  time and \(\ell_{1}\)-regularization on the initial function
	  values and on graph edge weights in time
	  \(\tilde{O}(m^{\frac{3}{2}})\).
	  </p>
	</div>
	</div><!--article ends-->





			<div class="article">
	<div class="title">Provable ICA with Unknown Gaussian Noise,
	and Implications <br/> for Gaussian Mixtures and Autoencoders</div>

	<div class="conf">
	   <a href="http://link.springer.com/journal/453/72/1/page/1"
	  target="_blank">Algorithmica 2015</a><br/>
	  <a
	href="https://nips.cc/Conferences/2012/"
	target="_blank">NIPS 2012</a>
	  </div>
	
	<div class="author_list">
	  with <span class="authors">Sanjeev Arora, Rong Ge, Ankur
	  Moitra</span>
	</div>

	<div class="link_list">
	    <div class="link">
	      <a href="javascript:toggle('abstract7')">Abstract</a>
	    </div>
	    <div class="link">
	     <a
	href="http://link.springer.com/article/10.1007/s00453-015-9972-2"
	      target="_blank">Journal</a>
	    </div>
	    <div class="link">
	      <a
	    href="http://papers.nips.cc/paper/4603-provable-ica-with-unknown-gaussian-noise-with-implications-for-gaussian-mixtures-and-autoencoders"
	      target="_blank">Conference</a>
	    </div>
	    <div class="link">
	      <a href="http://arxiv.org/abs/1206.5349"
	      target="_blank">arXiv</a>
	    </div>
	    
	</div>
	
	<div id="abstract7" class="abstract"> <p> We present a new
	  algorithm for Independent Component Analysis (ICA) which has
	  provable performance guarantees. In particular, suppose we
	  are given samples of the form y = Ax + \(\eta\) where A is
	  an unknown n X n matrix and x is chosen uniformly at random
	  from \(\{+1, -1\}^n\), \(\eta\) is an n-dimensional Gaussian
	  random variable with unknown covariance \(\Sigma\): We give
	  an algorithm that provable recovers A and \(\Sigma\) up to
	  an additive \(\epsilon\) whose running time and sample
	  complexity are polynomial in n and \(1 / \epsilon\). To
	  accomplish this, we introduce a novel "quasi-whitening" step
	  that may be useful in other contexts in which the covariance
	  of Gaussian noise is not known in advance. We also give a
	  general framework for finding all local optima of a function
	  (given an oracle for approximately finding just one) and
	  this is a crucial step in our algorithm, one that has been
	  overlooked in previous attempts, and allows us to control
	  the accumulation of error when we find the columns of A one
	  by one via local search.</p>
	</div>
	</div><!--article ends-->



			
	<div class="article">
	<div class="title">Finding Overlapping Communities in Social
	Networks:<br/>Toward a Rigorous Approach</div>
	
	<div class="conf">
	  <a  href="http://www.sigecom.org/ec12/schedule_conference.html" target="_blank">EC 2012</a>
	</div>

	<div class="author_list">
	with
	  <span class="authors">
	  Sanjeev Arora,
	  Rong Ge,
	  Grant Schoenebeck
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract5')">Abstract</a>
	</div>
	<div class="link">
	  <a href="http://dl.acm.org/citation.cfm?id=2229020" target="_blank">Conference</a>
	</div>
	<div class="link">
	  <a href="http://arxiv.org/pdf/1112.1831v1"
	  target="_blank">arXiv</a>
	  </div>
	</div>
	
	<div id="abstract5" class="abstract">
	  <p>
	  A community in a social network is usually understood to
	  be a group of nodes more densely connected with each other than with
	  the rest of the network. This is an important concept in most domains
	  where networks arise: social, technological, biological, etc. For many
	  years algorithms for ﬁnding communities implicitly assumed communities
	  are nonoverlapping (leading to use of clustering-based approaches) but
	  there is increasing interest in ﬁnding overlapping communities. A
	  barrier to ﬁnding communities is that the solution concept is often
	  deﬁned in terms of an NP-complete problem such as Clique or
	  Hierarchical Clustering.</p>

	  <p>
	  This paper seeks to initiate a rigorous
	  approach to the problem of ﬁnding overlapping communities, where
	  “rigorous” means that we clearly state the following: (a) the object
	  sought by our algorithm (b) the assumptions about the underlying
	  network (c) the (worst-case) running time.<br/>
	  Our assumptions about the
	  network lie between worst-case and average-case. An average-case
	  analysis would require a precise probabilistic model of the network,
	  on which there is currently no consensus. However, some plausible
	  assumptions about network parameters can be gleaned from a long body
	  of work in the sociology community spanning ﬁve decades focusing on
	  the study of individual communities and ego-centric networks (in graph
	  theoretic terms, this is the subgraph induced on a node’s
	  neighborhood). Thus our assumptions are somewhat “local” in
	  nature. Nevertheless they suﬃce to permit a rigorous analysis of
	  running time of algorithms that recover global
	  structure. </p>

	  <p>
	  Our
	  algorithms use random sampling similar to that in property testing and
	  algorithms for dense graphs. We note however that our networks are not
	  necessarily dense graphs, not even in local neighborhoods.  Our
	  algorithms explore a local-global relationship between ego-centric and
	  socio-centric networks that we hope will provide a fruitful
	framework for future work both in computer science and sociology.
	  </p>
	</div>
	</div><!--article ends-->

	
  </div>







  <div id="pubs" class="section">
    <H3>Hardness of Approximation</H3>

     	<div class="article">
	<div class="title">Inapproximability of Minimum Vertex Cover
	  on <br/>
	  k-Uniform k-Partite Hypergraphs</div>
	
	<div class="conf">
	  <a
	href="http://epubs.siam.org/toc/sjdmec/29/1"
	target="_blank">SIDMA 2015</a>
	</div>

	<div class="author_list">
	with
	  <span class="authors">
	  Venkatesan Guruswami,
	  Rishi Saket
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract14')">Abstract</a>
	</div>
	<div class="link">
	  <a href="http://epubs.siam.org/doi/abs/10.1137/130919416"
	  target="_blank">Journal</a>
	</div>
	<div class="link">
	  <a href="http://eccc.hpi-web.de/report/2013/071/" target="_blank">ECCC</a>
	  </div>
	</div>
	
	<div id="abstract14" class="abstract">
	  <p>
	  	  We study the problem of computing the minimum vertex cover
	  on k-uniform k-partite hypergraphs when the
	  k-partition is given. On bipartite graphs (k=2), the
	  minimum vertex cover can be computed in polynomial time. For
	  k\(\ge 3,\) this problem is known to be NP-hard. For general
	  k, the problem was studied by Lovász, who gave a
	  \(\frac{k}{2}\)-approximation based on the standard LP
	  relaxation. Subsequent work by Aharoni, Holzman, and
	  Krivelevich showed a tight integrality gap of \(\frac{k}{2}
	  - o(1))\) for the LP relaxation. We further investigate the
	  inapproximability of minimum vertex cover on k-uniform
	  k-partite hypergraphs and present the following results
	  (here \(\varepsilon  > 0\) is an arbitrarily small constant):
	  NP-hardness of obtaining an approximation factor of
	  \((\frac{k}{4} - \varepsilon)\) for even k and \((\frac{k}{4}
	  - \frac{1}{4k} - \varepsilon)\) for odd k, NP-hardness of
	  obtaining a nearly optimal approximation factor of
	  \((\frac{k}{2}-1+\frac{1}{2k}-\varepsilon)\), and an optimal
	  unique games-hardness for approximation within factor
	  \((\frac{k}{2} - \varepsilon)\), showing the optimality of
	  Lovász's algorithm if one assumes the Unique Games
	  conjecture. The first hardness result is based on a
	  reduction from minimum vertex cover in r-uniform
	  hypergraphs, for which NP-hardness of approximating within
	  \(r - 1 -\varepsilon\) was shown by Dinur, Guruswami, Khot,
	  and Regev. We include it for its simplicity, despite it
	  being subsumed by the second hardness result. The unique
	  games-hardness result is obtained by applying the results of
	  Kumar, Manokaran, Tulsiani, and Vishnoi, with a slight
	  modification, to the LP integrality gap due to Aharoni,
	  Holzman, and Krivelevich. The modification ensures that the
	  reduction preserves the desired structural properties of the
	  hypergraph. The reduction for the nearly optimal NP-hardness
	  result relies on the multilayered PCP of Dinur, Guruswami,
	  Khot, and Regev and uses a gadget based on biased long codes
	  adapted from the LP integrality gap of Aharoni, Holzman, and
	  Krivelevich. Our reduction requires the analysis of several
	  long codes with different biases, for which we prove
	  structural properties of the so-called cross-intersecting
	  collections of set families, variants of which have been
	  studied in extremal set theory
	  </p>
	</div>
	</div><!--article ends-->


 	<div class="article">
	<div class="title">
	Optimal Inapproximability for Scheduling Problems <br/> via
	  Structural Hardness for Hypergraph Vertex Cover
	</div>
	
      <div class="conf">
	<a
	href="http://computationalcomplexity.org/Archive/2013/cfp.html"
	target="_blank">CCC 2013</a>
      </div>

	<div class="author_list">
	with
	  <span class="authors">
	  Rishi Saket
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract10')">Abstract</a>
	</div>
	<div class="link">
	  <a
	  href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6597764"
	  target="_blank">Conference</a>
	</div>
	</div>
	
	<div id="abstract10" class="abstract">
	  <p>This work studies the inapproximability of Minimum Vertex Cover on
	  uniform hypergraphs from a specific structural perspective. Our
	  study is motivated by applications to two well known scheduling
	  problems: <i>Concurrent Open Shop</i> and the <i>Assembly Line</i>
	  problem. For both these problems, Bansal and Khot [BK10]
	  obtained tight \((2-\epsilon)\)-factor inapproximability, assuming the
	  Unique Games Conjecture (UGC). In this work, we prove optimal
	  \((2-\epsilon)\)-factor NP-hardness of approximation for both these
	  problems <i>unconditionally</i>, <i>i.e.</i>, without assuming UGC.</p>

	  <p>Our results for the scheduling problems follow from a structural
	  hardness for Minimum Vertex Cover on hypergraphs -- an unconditional
	  but weaker analog of a similar result of Bansal and
	  Khot [BK10] which however, is based on UGC.
	  </p>
	</div>
	</div><!--article ends-->


		<div class="article">
	<div class="title">Nearly Optimal NP-Hardness of Vertex Cover
	on 
	\(k\)-Uniform <br/> \(k\)-Partite Hypergraphs</div>
	
      <div class="conf">
	<a
	href="http://cui.unige.ch/tcs/random-approx/2011/index.php"
	target="_blank">Approx 2011</a>
      </div>

	<div class="author_list">
	with
	  <span class="authors">
	  Rishi Saket
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract4')">Abstract</a>
	</div>
	<div class="link">
	  <a
	  href="http://link.springer.com/chapter/10.1007%2F978-3-642-22935-0_28"
	  target="_blank">Conference</a>
	</div>
	<div class="link">
	  <a href="http://arxiv.org/abs/1105.4175" target="_blank">arXiv</a>
	</div>
	</div>
	
	<div id="abstract4" class="abstract">
	  <p>
	  	  We study the problem of computing the minimum vertex cover on
	  \(k\)-uniform \(k\)-partite hypergraphs when the \(k\)-partition is given.
	  On bipartite graphs \((k=2)\), the minimum vertex cover can be
	  computed in polynomial time.  For general \(k\), the problem was
	  studied by Lov&aacutesz, who gave a
	  \(\frac{k}{2}\)-approximation based on the standard LP
	  relaxation. Subsequent work by Aharoni, Holzman and
	  Krivelevich showed a tight integrality gap of
	  \(\frac{k}{2} - o(1)\) for the LP relaxation. 
	  While this problem was
	  known to be NP-hard for \(k\geq 3\), the first non-trivial NP-hardness
	  of approximation factor of \(\frac{k}{4}-\epsilon\) was shown in a recent
	  work by Guruswami and Saket. They also showed that
	  assuming Khot's Unique Games Conjecture yields a \(\frac{k}{2}-\epsilon\)
	  inapproximability for this problem, implying the optimality of
	  Lov&aacutesz's result.</p>
	  
	  <p>
	  In this work, we 
	  show that this problem is NP-hard to approximate within 
	  \(\frac{k}{2}-1+\frac{1}{2k}-\epsilon\). This hardness factor 
	  is off from the optimal
	  by an additive constant of at most 1 for \(k\geq
	  4\). Our reduction relies on the <it>Multi-Layered PCP</it> of
	  Dinur et al. and uses a gadget -- based on
	  biased Long Codes -- adapted from the LP integrality gap of Aharoni et
	  al. 
	  The nature of our reduction
	  requires the analysis of several Long Codes with different biases, for
	  which we prove structural properties of the so called
	  cross-intersecting collections of set families -- 
	  variants of which have
	  been studied in extremal set theory.</p>    
	  </p>
	</div>
	</div><!--article ends-->


		<div class="article">
	<div class="title">Cuts in Cartesian Products of Graphs</div>
	
	<div class="conf">
	  Manuscript
	</div>

	<div class="author_list">
	with
	  <span class="authors">
	  Madhur Tulsiani
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract3')">Abstract</a>
	</div>
	<div class="link">
	  <a href="http://arxiv.org/abs/1105.3383" target="_blank">arXiv</a>
	</div>
	<div class="link">
	  <a href="https://youtu.be/FILqdT-0XCM"
	    target="_blank">Video</a>
	</div>
	</div>
	
	<div id="abstract3" class="abstract">
	  <p>
	  The \(k\)-fold Cartesian product of a graph \(G\) is defined
	    as a graph on tuples \((x_1,\ldots,x_k)\) where two tuples
	    are connected if they form an edge in one of the positions
	    and are equal in the rest. Starting with \(G\) as a single
	    edge gives \(G^{\Box k}\) as a \(k\)-dimensional
	    hypercube. We study the distributions of edges crossed by
	    a cut in \(G^{\Box k}\) across the copies of \(G\) in
	    different positions. This is a generalization of the
	    notion of influences for cuts on the hypercube.</p>
	    
	    <p>We show the analogues of the statements of Kahn, Kalai
	    and Linial (KKL theorem) and that of Friedgut (Friedgut's
	    Junta Theorem), for the setting of Cartesian products of
	    arbitrary graphs. We also extends the work on studying
	    isoperimetric constants for these graphs to the value of
	    semidefinite relaxations for expansion.  We connect the
	    optimal values of the relaxations for computing expansion,
	    given by various semidefinite hierarchies, for \(G\) and
	    \(G^{\Box k}\).
	  </p>
	</div>
	</div><!--article ends-->

	
  </div>

  <div id="pubs" class="section">    
    <H3>Other Publications</H3>
     


	


        <div class="article">
    <div class="title">Near-optimal approximation algorithm for
    simultaneous Max-Cut</div>
	
	
      <div class="conf">
	<a href="http://www.dcs.warwick.ac.uk/~czumaj/SODA_2018_List_of_accepted_papers.htm"
	target="_blank">SODA 2018
	</a>
	</div>

	<div class="author_list">
	with
	<span class="authors">
	  Amey Bhangale, Subhash Khot, Swastik Kopparty, Devanathan
	  Thiruvenkatachari</span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract24')">Abstract</a>
	</div>
	<div class="link">
	  <a
	  href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611975031.93"
	  target="_blank">Conference</a>
	</div>
	<div class="link">
	  <a
	  href="https://arxiv.org/abs/1801.04497"
	  target="_blank">arXiv</a>
	</div>
	</div>
	
	<div id="abstract24" class="abstract"> <p> In the simultaneous
	    Max-Cut problem, we are given \(k\) weighted graphs on the
	    same set of \(n\) vertices, and the goal is to find a cut
	    of the vertex set so that the minimum, over the \(k\)
	    graphs, of the cut value is as large as possible. Previous
	    work [BKS15] gave a polynomial time algorithm which
	    achieved an approximation factor of \(\frac{1}{2}-o(1)\)
	    for this problem (and an approximation factor of
	    \(\frac{1}{2} + \epsilon_k\) in the unweighted case, where
	    \(\epsilon_k \to 0\) as \(k \to \infty\)).<br/>  In this
	    work, we give a polynomial time approximation algorithm
	    for simultaneous Max-Cut with an approximation factor of
	    0.8780 (for all constant \(k\)). The natural SDP
	    formulation for simultaneous Max-Cut was shown to have an
	    integrality gap of \(\frac{1}{2} + \epsilon_k\) in
	    [BKS15]. In achieving the better approximation guarantee,
	    we use a stronger Sum-of-Squares hierarchy SDP relaxation
	    and a rounding algorithm based on Raghavendra-Tan [RT12],
	    in addition to techniques from [BKS15].
	  </p>
	</div> </div><!--article ends-->

	





	
	<div class="article">
	<div class="title">Simultaneous
	Approximation of Constraint Satisfaction Problems</div>
	
	<div class="conf">
	  <a
	  href="http://www.kurims.kyoto-u.ac.jp/icalp2015/accepted-ICALP-A.html"
	  target="_blank">ICALP 2015</a>
	</div>

	<div class="author_list">
	with
	  <span class="authors">
	  Amey Bhangale,
	  Swastik Kopparty
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract15')">Abstract</a>
	</div>
	<div class="link">
	  <a
	  href="http://link.springer.com/chapter/10.1007/978-3-662-47672-7_16"
	  target="_blank">Conference</a>
	</div>
	<div class="link">
	  <a href="http://arxiv.org/abs/1407.7759"
	    target="_blank">arXiv</a>
	</div>
	<div class="link">
	  <a href="http://eccc.hpi-web.de/report/2014/098/"
	    target="_blank">ECCC</a>
	</div>
	</div>
	
	<div id="abstract15" class="abstract">
	  <p>
	  Given k collections
	  of 2SAT clauses on the same set of variables V, can we find
	  one assignment that satisfies a large fraction of clauses
	  from <it>each</it> collection? We consider such
	  <it>simultaneous</it> constraint satisfaction problems, and
	  design the first nontrivial approximation algorithms in this
	  context. <p/>

	  <p>Our main result is that for every CSP F, for \(k <
	  \tilde{O}(\log^{\frac{1}{4}} n)\), there is a polynomial time
	  constant factor <it>Pareto</it> approximation algorithm for k
	  simultaneous Max-F-CSP instances. Our methods are quite
	  general, and we also use them to give an improved
	  approximation factor for simultaneous Max-w-SAT (for \(k
	  <\tilde{O}(\log^{\frac{1}{3}} n)\)).  In contrast, for \(k =
	  \omega(\log n)\), no nonzero approximation factor for k
	  simultaneous Max-F-CSP instances can be achieved in polynomial
	  time (assuming the Exponential Time Hypothesis).
	  </p>
	  
	  <p>These problems are a natural meeting point for the theory of
	  constraint satisfaction problems and multiobjective
	  optimization.  We also suggest a number of interesting
	  directions for future research.
	  </p>
	</div>
	</div><!--article ends-->


	
	<div class="article">
	<div class="title"> New Results in the Theory of
	  Approximation:<br/>
	  Fast Graph Algorithms and Inapproximability</div>
	
	<div class="conf">
	  Thesis<br/>
	  Sept 2013
	</div>


	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract12')">Abstract</a>
	</div>
	<div class="link">
	  <a href="pubs/thesis.pdf"
	  target="_blank">PDF</a>
	</div>
	</div>
	
	<div id="abstract12" class="abstract">
	  <p>
	  For several basic optimization problems, it is NP-hard to
	    find an exact solution. As a result, understanding the
	    best possible trade-off between the running time of an
	    algorithm and its approximation guarantee, is a
	    fundamental question in theoretical computer science, and
	    the central goal of the theory of approximation.</p>

	  <p>There are two aspects to the theory of approximation :
	  (1) efficient approximation algorithms that establish
	  trade-offs between approximation guarantee and running time,
	  and (2) inapproximability results that give evidence against
	  them. In this thesis, we contribute to both facets of the
	  theory of approximation.</p>



	  <p>In the first part of this thesis, we present the first
	  near-linear-time algorithm for Balanced Separator - given a graph,
	  partition its vertices into two roughly equal parts without cutting
	  too many edges - that achieves the best approximation guarantee
	  possible for algorithms in its class.  This is a classic graph
	  partitioning problem and has deep connections to several areas of both
	  theory and practice, such as metric embeddings, Markov chains,
	  clustering, etc.</p>

	  <p>As an important subroutine for our algorithm for Balanced
	  Separator, we provide a near-linear-time algorithm to
	  simulate the heat-kernel random walk on a graph, equivalent
	  to computing \(\exp(-L)v\), where \(L\) is the Laplacian of
	  the graph, and \(v\) is a vector. This algorithm combines
	  techniques from approximation theory and numerical linear
	  algebra to reduce the problem of approximating the matrix
	  exponential to solving a small number of Laplacian
	  systems. We also give a reduction in the reverse direction,
	  from matrix inversion to matrix exponentiation, hence
	  justifying the use of Laplacian system solvers.</p>

	  <p>In the second part of this thesis, we prove
	  inapproximability results for several basic optimization
	  problems. We address some classic scheduling problems,
	  <it>viz.</it> Concurrent Open Shop and the Assembly Line
	  problem, and variants of the Hypergraph Vertex Cover
	  problem. For all these problems, optimal inapproximability
	  results were previously known under the Unique Games
	  Conjecture. We are able to prove near-optimal
	  inapproximability results for these problems without using
	  the conjecture.  </p> </div> </div>


	  
	<div class="article">
	<div class="title">
	  An  Arithmetic Analogue of Fox's Triangle Removal Argument
	</div>

	<div class="conf">
	  <a href="http://www.math.rochester.edu/ojac/articles.html"
	  target="_blank">
	  OJAC</a>
	</div>

	<div class="author_list">
	with
	  <span class="authors">
	  Pooya Hatami,
	  Madhur Tulsiani
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract11')">Abstract</a>
	</div>
	<div class="link">
	  <a href="http://www.math.rochester.edu/ojac/vol11/124.pdf" target="_blank">Journal</a>
	</div>
	<div class="link">
	  <a href="http://arxiv.org/abs/1304.4921" target="_blank">arXiv</a>
	</div>
	<div class="link">
	  <a href="https://video.ias.edu/csdm/1213/0402-SushantSachdeva"
	    target="_blank">Video</a>
	</div>
	</div>
	
	<div id="abstract11" class="abstract">
	  <p> We give an arithmetic
	  version of the recent proof of the triangle removal lemma by
	  Fox [Fox11], for the group \(\mathbb{F}_2^n\).</p>

	  <p> A
	  triangle in \(\mathbb{F}_2^n\) is a tuple \((x,y,z)\) such
	  that \(x+y+z = 0\). The triangle removal lemma for
	  \(\mathbb{F}_2^n\) states that for every \(\epsilon > 0\)
	  there is a \(\delta > 0\), such that a subset \(A\) of
	  \(\mathbb{F}_2^n\) which is \(\epsilon\)-far from being
	  triangle free, must contain at least \(\delta \cdot 2^{2n}\)
	  triangles. We give a direct proof which gives an improved
	  lower bound for \(\delta\), analogous to the one obtained by
	  Fox for triangle removal in graphs.
	  </p>

	  <p>The improved lower
	  bound was already known to follow (for triangle-removal in
	  all groups), using Fox's removal lemma for directed cycles
	  and a reduction by Kral, Serra and Vena. The purpose of this
	  note is to provide a direct Fourier-analytic proof for the
	  group \(\mathbb{F}_2^n\).  </p>
	</div>
	</div><!--article  ends-->

	


	

	<div class="article">
	<div class="title">Testing Permanent Oracles - Revisited</div>
	
	<div class="conf">
	  <a
	  href="http://cui.unige.ch/tcs/random-approx/2012/index.php?id=9"
	  target="_blank">Random 2012</a>
	</div>

	<div class="author_list">
	with
	  <span class="authors">
	  Sanjeev Arora,
	  Arnab Bhattacharyya,
	  Rajsekar Manokaran
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract6')">Abstract</a>
	</div>
	<div class="link">
	  <a
	  href="http://link.springer.com/chapter/10.1007%2F978-3-642-32512-0_31"
	  target="_blank">Conference</a>
	</div>
	<div class="link">
	  <a href="http://arxiv.org/abs/1207.4783"
	target="_blank">arXiv</a>
	</div>
	<div class="link">
	  <a href="http://eccc.hpi-web.de/report/2012/094/" target="_blank">ECCC</a>
	</div>
	</div>
	
	<div id="abstract6" class="abstract">
	  <p>
	   Suppose we are given an oracle that claims to approximate
	  the permanent for most matrices X, where X is chosen from the
	  <i>Gaussian ensemble</i> the matrix entries are i.i.d.~univariate
	  complex Gaussians).  Can we test that the oracle satisfies this claim?
	  This paper gives a polynomial-time algorithm for the task.</p>
	  
	  <p>The oracle-testing problem is of interest because a recent paper of
	  Aaronson  and Arkhipov  showed that  if there  is  a polynomial-time
	  algorithm  for   simulating  boson-boson  interactions   in  quantum
	  mechanics, then  an approximation oracle  for the permanent  (of the
	  type  described   above)  exists  in BPP^NP.  Since  computing
	  the permanent of  even 0/1 matrices is  #P-complete, this seems
	  to demonstrate  more computational  power in quantum  mechanics than
	  Shor's factoring algorithm does. However, unlike factoring, which is
	  in NP, it was unclear previously how to test  the   correctness
	  of  an  approximation   oracle  for  the   permanent, and this is
	  the contribution of the paper. </p>

	  <p>The technical difficulty overcome here is that univariate polynomial
	  self-correction, which  underlies similar oracle-testing algorithms
	  for permanent over<i> finite fields</i> ---and whose discovery led to
	  a  revolution in complexity  theory---does not  seem to  generalize to
	  complex (or even, real) numbers.   We believe that
	  this  tester will  motivate further  progress on  understanding the
	  permanent of Gaussian matrices.
	  </p>
	</div>
	</div><!--article ends-->


	


		<div class="article">
	<div class="title">The Power of Weak v/s Strong Triangle Inequalities</div>
	
	<div class="conf">
	  Manuscript
	</div>

	<div class="author_list">
	with
	  <span class="authors">
	  Mohammad Moharrami
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract2')">Abstract</a>
	</div>
	<div class="link">
	  <a href="pubs/triangle-ineq.pdf"
	  target="_blank">PDF</a>
	</div>
	</div>

	
	
	<div id="abstract2" class="abstract">
	  <p>
	  In a recent paper, Lee and Moharrami construct a family of
	    metrics \((X_n,d_n)\) with \(|X_n|=N\), such that
	    \((X_n,\sqrt{d_n})\) embeds into \(\ell_2\) with constant
	    distortion, but embedding \((X_n,d_n)\) into a metric of
	    negative type requires distortion \(\Omega((\log
	    N)^{\frac{1}{4}})\). In this paper, we build on their
	    analysis, and improve their result by showing a
	    \(\Omega((\log N)^{\frac{1}{3}})\) lower bound for
	    embedding \((X_n,d_n)\) into a metric of negative
	    type. Moreover, we show that this analysis is essentially
	    tight by constructing a map that has distortion \(O((\log
	    N)^{\frac{1}{3}})\).</p>
	    
	    <p>This result implies a lower bound of \(\Omega((\log N)^{\frac{1}{3}})\) for the
	    integrality gap of the relaxed version of Goemans-Linial semidefinite program
	    with weak triangle inequalities for Non-uniform Sparsest Cut.
	  </p>
	</div>
	</div><!--article ends-->


	<div class="article">
	<div class="title">A Reformulation of the Arora-Rao-Vazirani Structure Theorem</div>
	
	<div class="conf">
	  Manuscript
	</div>

	<div class="author_list">
	with
	  <span class="authors">
	  Sanjeev Arora,
	  James Lee
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract1')">Abstract</a>
	</div>
	<div class="link">
	   <a href="http://arxiv.org/abs/1102.1456"
	  target="_blank">arXiv</a>
	</div>
	</div>
	
	<div id="abstract1" class="abstract">
	  <p>
	   In a well-known paper, Arora, Rao and Vazirani obtained an
	  \(O(\sqrt{\log n})\) approximation to the Balanced Separator
	  problem and Uniform Sparsest Cut. At the heart of their
	  result is a geometric statement about sets of points that
	  satisfy triangle inequalities, which also underlies
	  subsequent work on approximation algorithms and geometric
	  embeddings.  In this note, we give an equivalent formulation
	  of the Structure theorem in [ARV] in terms of the expansion
	  of large sets in geometric graphs on sets of points
	  satisfying triangle inequalities.
	  </p>
	</div>
	</div><!--article ends-->



<!--
	<div class="article">
	<div class="title"></div>
	
      <div class="conf">
	</div>

	<div class="author_list">
	with
	<span class="authors">
	  </span>
	</div>

	<div class="link_list">
	<div class="link">
	    <a href="javascript:toggle('abstract')">Abstract</a>
	</div>
	<div class="link"> 
	</div>
	</div>
	
	<div id="abstract" class="abstract">
	  <p>
	  </p>
	</div>
	</div>
	-->

	
      </div><!--pubs end-->
    <div class="section">
    </div>
      
</div> <!--content end-->
</div> <!--container end-->
</body>

</html>
